{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0f01cb",
   "metadata": {},
   "source": [
    "# World Bank Financial Survey Q&A Model Project \n",
    "\n",
    "This project develops a NLP powererd question-answering system that is trained on World Bank Survey Data containing financial information gathered from various federal banks across the globe. This notebook walks the user through gathering/processing the data and training/deploying the final model. \n",
    "\n",
    "### Dataset Description\n",
    "The World Bank survey dataset comprises of structured financial questions sent to financial instituitions worldwide. The dataset includes multi-dimensional survey responses, hierarchial question structures, and financial metrics. For this project, we will use the questions that have long-form textual answers to train an NLP model, rather than using binary response questions.\n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "##### Phase 1 - Data Processing \n",
    "\n",
    "- Transform unstructured survey data into structurerd NLP training pairs\n",
    "    - Parse all relevant sheets from excel file\n",
    "    - Properly handle hierarchical question structures to ensure each question answer pair is standalone\n",
    "- Indentify and Flag PII using a ML model in dataset\n",
    "\n",
    "##### Phase 2 - Model Development & Fine Tuning\n",
    "\n",
    "- Fine-tune a Google FLAN-T5-Base NLP model \n",
    "- Optimize the model's performance on this specific World Bank survey domain\n",
    "- Evaluate model performance using validation and test sample sets\n",
    "\n",
    "##### Phase 4 - Deployement\n",
    "\n",
    "- Deploy fine-tuned model to production environment on Azure/Huggingface\n",
    "\n",
    "##### Future Steps (if time allows):\n",
    "- implement API for interacting with model\n",
    "- add some sort of sentiment analysis to clasify questions/answers (financial questions, admin questions, etc)\n",
    "- get feedback on model performance (answer quality/hallucinations/knowledge gaps)  \n",
    "- add additional survey questions to knowledge base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfe9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from World Bank Database\n",
    "import requests\n",
    "\n",
    "url = \"https://datacatalogfiles.worldbank.org/ddh-published/0038632/2/DR0047737/2021_04_26_brss-public-release.xlsx\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"worldbank_data.xlsx\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2654b84",
   "metadata": {},
   "source": [
    "Now that data is downloaded, it needs to be converted from an xlsx file with row column format to something that works for t5 training (question:answer pairs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea782ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample questions after simplification:\n",
      "0: Answer this question about Albania: 1.1 What body/agency grants banking licenses?...\n",
      "1: Answer this question about Angola: 1.1 What body/agency grants banking licenses?...\n",
      "2: Answer this question about Antigua and Barbuda: 1.1 What body/agency grants banking licenses?...\n",
      "3: Answer this question about Argentina: 1.1 What body/agency grants banking licenses?...\n",
      "4: Answer this question about Armenia: 1.1 What body/agency grants banking licenses?...\n"
     ]
    }
   ],
   "source": [
    "## read and process data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Remove extra unnecessary information from question\n",
    "# For example, \"Select all that apply\"\n",
    "def simplify_question(qText):\n",
    "    if pd.isna(qText):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(qText).strip()\n",
    "    \n",
    "    # split on common instruction starters and take first part\n",
    "    for splitter in [\" Please \", \" If \", \" Include \", \" Specify \", \" Describe \", \" List \"]:\n",
    "        if splitter in text:\n",
    "            text = text.split(splitter)[0]\n",
    "            break\n",
    "    \n",
    "    # if there's a question mark, take up to first one\n",
    "    if \"?\" in text:\n",
    "        text = text.split(\"?\")[0] + \"?\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# loads all sheets at once\n",
    "allSheets = pd.ExcelFile(\"worldbank_data.xlsx\")\n",
    "\n",
    "# store samples\n",
    "samples = []\n",
    "\n",
    "# process all sheets except first 2 and last 1\n",
    "process = allSheets.sheet_names[2:-1]\n",
    "\n",
    "# read first sheet and extract countries\n",
    "dfFirst = pd.read_excel(allSheets, sheet_name=process[0], header=None)\n",
    "countries = [str(c) for c in dfFirst.iloc[0, 2:].values if not pd.isna(c)]\n",
    "\n",
    "for sheet in process:\n",
    "    # read current sheet\n",
    "    df = pd.read_excel(allSheets, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # create parent and base vars\n",
    "    parent = None\n",
    "    currBase = None\n",
    "    \n",
    "    # iterate through every row except header\n",
    "    # get question index and question text\n",
    "    for idx, row in df.iloc[1:].iterrows():\n",
    "        qIndex = row[0]\n",
    "        qText = row[1]\n",
    "        \n",
    "        # if the question index is null but text does exist \n",
    "        # then the question is a parent question\n",
    "        # assign parent question and then clear prev base and move onto next row\n",
    "        if pd.isna(qIndex) and not pd.isna(qText):\n",
    "            parent = simplify_question(qText)  # ← Simplify parent too\n",
    "            currBase = None\n",
    "            continue\n",
    "        \n",
    "        # regex starts with Q and captures groups delimited by _\n",
    "        # group 1 is the main question number\n",
    "        # group 2 is sub-question number\n",
    "        # group 3 is for multi-part questions with extra text\n",
    "        # non-capturing group is for sections of index which are unnecessary\n",
    "        match = re.match(r'Q(\\d+)_([0-9_]+?)([a-zA-Z_]+)?(?:_[A-Z]|_\\d{4}|$)', str(qIndex))\n",
    "        \n",
    "        # if regex matched then process row, otherwise skip\n",
    "        if match:\n",
    "            baseNum = f\"{match.group(1)}_{match.group(2)}\"\n",
    "            isMulti = bool(match.group(3)) or bool(re.search(r'_\\d{4}', str(qIndex)))\n",
    "            part = match.group(3) if match.group(3) else \"\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # if new base is different to current base, update base\n",
    "        if baseNum and baseNum != currBase:\n",
    "            # reset parent if new question isn't multi part\n",
    "            if not isMulti:\n",
    "                parent = None\n",
    "            currBase = baseNum\n",
    "        \n",
    "        # loop through each column\n",
    "        for colIdx, country in enumerate(countries):\n",
    "            \n",
    "            # get answer for current column\n",
    "            answer = row[colIdx + 2]\n",
    "            \n",
    "            # skip column if there's no answer\n",
    "            if pd.isna(answer):\n",
    "                continue\n",
    "            \n",
    "            # Simplify the question text\n",
    "            simplifiedQ = simplify_question(qText)  # ← KEY CHANGE\n",
    "            \n",
    "            # if question is multi-part combine parent question and question text\n",
    "            if isMulti and parent:\n",
    "                completeQ = f\"{parent} {simplifiedQ}\"\n",
    "            # otherwise just append question text\n",
    "            else:\n",
    "                completeQ = simplifiedQ\n",
    "            \n",
    "            # fill in sample entry\n",
    "            sample = {\n",
    "                \"input\": f\"Answer this question about {country}: {completeQ}\".strip(),\n",
    "                \"target\": str(answer).strip()\n",
    "            }\n",
    "            \n",
    "            # append sample to list\n",
    "            samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920245f",
   "metadata": {},
   "source": [
    "Now that the data is in proper training format, it needs to be checked for PII. We will use Microsoft's Presidio pre-trained ML library to detect PII (https://github.com/microsoft/presidio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependecies\n",
    "# !pip install presidio_analyzer presidio_anonymizer\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb74525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding pii: 100%|██████████| 107833/107833 [35:25<00:00, 50.73it/s] \n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# initialize analyzer\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# specific countries and years are necessary to the survey data\n",
    "# do not flag these as PII\n",
    "excludeWords = set(countries)\n",
    "excludeWords.update(['2011', '2012', '2013', '2014', '2015', '2016'])\n",
    "\n",
    "# only include entries that the model has 70%+ confidnece is PII\n",
    "CONFIDENCE = 0.7\n",
    "\n",
    "# only track unique PII values\n",
    "seenPII = set()\n",
    "\n",
    "# storage for PII\n",
    "potentialPII = []\n",
    "\n",
    "# iterate through every sample\n",
    "for idx, sample in enumerate(tqdm(samples, desc='finding pii')):\n",
    "\n",
    "    # get input question and target\n",
    "    inputText = sample[\"input\"]\n",
    "    targetText = sample[\"target\"]\n",
    "\n",
    "    # analyze input and target\n",
    "    inputRes = analyzer.analyze(text=inputText, language='en')\n",
    "    targetRes = analyzer.analyze(text=targetText, language='en')\n",
    "\n",
    "    # filter out exclude list from text matches\n",
    "    inputRes = [r for r in inputRes \n",
    "                if r.score >= CONFIDENCE\n",
    "                and not any(inputText[r.start:r.end] in word or word in inputText[r.start:r.end] for word in excludeWords)] \n",
    "    targetRes = [r for r in targetRes \n",
    "                 if r.score >= CONFIDENCE \n",
    "                 and not any(targetText[r.start:r.end] in word or word in targetText[r.start:r.end] for word in excludeWords)]\n",
    "\n",
    "    # if pii is found\n",
    "    isNewPII = False\n",
    "    for r in inputRes:\n",
    "        if inputText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(inputText[r.start:r.end])\n",
    "    for r in targetRes:\n",
    "        if targetText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(targetText[r.start:r.end])\n",
    "\n",
    "    if isNewPII:\n",
    "        res = {\n",
    "            \"input\": inputText,\n",
    "            \"target\": targetText,\n",
    "            \"inputPII\": [{\"type\": r.entity_type, \"text\": inputText[r.start:r.end], \"score\": r.score} for r in inputRes],\n",
    "            \"targetPII\": [{\"type\": r.entity_type, \"text\": targetText[r.start:r.end], \"score\": r.score} for r in targetRes]\n",
    "        }\n",
    "        potentialPII.append(res)\n",
    "\n",
    "# dump all potential flagged PII into a json file\n",
    "with open('potentialPII.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(potentialPII, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa1d82",
   "metadata": {},
   "source": [
    "The code dumps all potential PII matches to a seperate JSON file saved to the current directory (potentiallyPII.json). This file can now be manually checked to determine which flagged keywords are false postives and which are actually PII. Once all PII is removed from the dataset, the T5 model training can begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   0%|          | 1/2198 [00:01<1:13:09,  2.00s/it, loss=3.2753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss: 3.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   5%|▍         | 101/2198 [01:34<31:00,  1.13it/s, loss=3.9937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100, Loss: 3.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:   9%|▉         | 201/2198 [03:05<30:01,  1.11it/s, loss=3.8435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200, Loss: 3.8435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  14%|█▎        | 301/2198 [04:35<28:43,  1.10it/s, loss=3.7290]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 300, Loss: 3.7290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  18%|█▊        | 401/2198 [06:05<27:17,  1.10it/s, loss=5.0440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 400, Loss: 5.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  23%|██▎       | 501/2198 [07:37<25:44,  1.10it/s, loss=3.7273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500, Loss: 3.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  27%|██▋       | 601/2198 [09:09<24:01,  1.11it/s, loss=3.1444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 600, Loss: 3.1444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  32%|███▏      | 701/2198 [10:39<23:04,  1.08it/s, loss=2.8311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 700, Loss: 2.8311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  36%|███▋      | 801/2198 [12:10<20:46,  1.12it/s, loss=2.3301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 800, Loss: 2.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  41%|████      | 901/2198 [13:42<19:47,  1.09it/s, loss=2.9432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 900, Loss: 2.9432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  46%|████▌     | 1001/2198 [15:12<18:41,  1.07it/s, loss=0.5565]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000, Loss: 0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  50%|█████     | 1101/2198 [16:43<16:43,  1.09it/s, loss=3.8108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100, Loss: 3.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  55%|█████▍    | 1201/2198 [18:12<14:47,  1.12it/s, loss=3.0865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200, Loss: 3.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  59%|█████▉    | 1301/2198 [19:44<13:39,  1.09it/s, loss=0.7979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300, Loss: 0.7979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  64%|██████▎   | 1401/2198 [21:15<11:56,  1.11it/s, loss=3.1600]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400, Loss: 3.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  68%|██████▊   | 1501/2198 [22:45<10:23,  1.12it/s, loss=2.7091]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500, Loss: 2.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  73%|███████▎  | 1601/2198 [24:16<09:02,  1.10it/s, loss=3.4244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600, Loss: 3.4244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  77%|███████▋  | 1701/2198 [25:48<07:47,  1.06it/s, loss=3.4210]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1700, Loss: 3.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  82%|████████▏ | 1801/2198 [27:19<05:53,  1.12it/s, loss=3.9049]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1800, Loss: 3.9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  86%|████████▋ | 1901/2198 [28:48<04:28,  1.11it/s, loss=2.5346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1900, Loss: 2.5346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  91%|█████████ | 2001/2198 [30:19<02:56,  1.12it/s, loss=1.5541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000, Loss: 1.5541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7:  96%|█████████▌| 2101/2198 [31:51<01:29,  1.09it/s, loss=3.2844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2100, Loss: 3.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/7: 100%|██████████| 2198/2198 [33:20<00:00,  1.10it/s, loss=2.4665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Avg Train Loss: 3.1506\n",
      "Epoch 1 - Avg Val Loss: 2.5105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   0%|          | 1/2198 [00:00<34:19,  1.07it/s, loss=3.9791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss: 3.9791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   5%|▍         | 101/2198 [01:31<32:33,  1.07it/s, loss=3.0512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100, Loss: 3.0512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:   9%|▉         | 201/2198 [03:01<30:25,  1.09it/s, loss=2.7804]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200, Loss: 2.7804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  14%|█▎        | 301/2198 [04:32<28:12,  1.12it/s, loss=3.2976]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 300, Loss: 3.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  18%|█▊        | 401/2198 [06:04<26:59,  1.11it/s, loss=2.5305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 400, Loss: 2.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  23%|██▎       | 501/2198 [07:36<26:46,  1.06it/s, loss=3.0669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500, Loss: 3.0669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  27%|██▋       | 601/2198 [09:06<23:41,  1.12it/s, loss=2.6273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 600, Loss: 2.6273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  32%|███▏      | 701/2198 [10:36<22:51,  1.09it/s, loss=3.3413]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 700, Loss: 3.3413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  36%|███▋      | 801/2198 [12:06<21:59,  1.06it/s, loss=3.1580]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 800, Loss: 3.1580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  41%|████      | 901/2198 [13:35<19:30,  1.11it/s, loss=2.2706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 900, Loss: 2.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  46%|████▌     | 1001/2198 [15:04<18:17,  1.09it/s, loss=2.0621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000, Loss: 2.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  50%|█████     | 1101/2198 [16:35<16:19,  1.12it/s, loss=0.3067]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100, Loss: 0.3067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  55%|█████▍    | 1201/2198 [18:04<15:12,  1.09it/s, loss=2.7249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200, Loss: 2.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  59%|█████▉    | 1301/2198 [19:35<14:00,  1.07it/s, loss=3.9300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300, Loss: 3.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  64%|██████▎   | 1401/2198 [21:05<11:49,  1.12it/s, loss=2.5581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400, Loss: 2.5581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  68%|██████▊   | 1501/2198 [22:35<10:09,  1.14it/s, loss=3.6079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500, Loss: 3.6079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  73%|███████▎  | 1601/2198 [24:03<08:41,  1.14it/s, loss=2.4546]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600, Loss: 2.4546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  77%|███████▋  | 1701/2198 [25:32<07:16,  1.14it/s, loss=1.7568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1700, Loss: 1.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  82%|████████▏ | 1801/2198 [27:02<05:47,  1.14it/s, loss=2.5789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1800, Loss: 2.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  86%|████████▋ | 1901/2198 [28:32<04:27,  1.11it/s, loss=2.0056]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1900, Loss: 2.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  91%|█████████ | 2001/2198 [30:16<02:59,  1.10it/s, loss=3.1382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000, Loss: 3.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7:  96%|█████████▌| 2101/2198 [31:47<01:26,  1.13it/s, loss=2.6823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2100, Loss: 2.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/7: 100%|██████████| 2198/2198 [33:15<00:00,  1.10it/s, loss=1.9555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Avg Train Loss: 2.5054\n",
      "Epoch 2 - Avg Val Loss: 2.2890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   0%|          | 1/2198 [00:00<33:52,  1.08it/s, loss=3.1608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss: 3.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   5%|▍         | 101/2198 [01:29<30:26,  1.15it/s, loss=1.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100, Loss: 1.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:   9%|▉         | 201/2198 [02:58<29:26,  1.13it/s, loss=0.8652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200, Loss: 0.8652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  14%|█▎        | 301/2198 [04:26<28:59,  1.09it/s, loss=2.5668]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 300, Loss: 2.5668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  18%|█▊        | 401/2198 [05:55<26:01,  1.15it/s, loss=2.3368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 400, Loss: 2.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  23%|██▎       | 501/2198 [07:23<24:25,  1.16it/s, loss=2.6552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500, Loss: 2.6552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  27%|██▋       | 601/2198 [08:51<23:32,  1.13it/s, loss=0.8995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 600, Loss: 0.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  32%|███▏      | 701/2198 [10:20<22:15,  1.12it/s, loss=3.4529]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 700, Loss: 3.4529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  36%|███▋      | 801/2198 [11:49<20:12,  1.15it/s, loss=1.1801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 800, Loss: 1.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  41%|████      | 901/2198 [13:20<19:29,  1.11it/s, loss=2.9472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 900, Loss: 2.9472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  46%|████▌     | 1001/2198 [14:52<17:44,  1.12it/s, loss=2.4347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000, Loss: 2.4347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  50%|█████     | 1101/2198 [16:28<19:36,  1.07s/it, loss=2.9001]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100, Loss: 2.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  55%|█████▍    | 1201/2198 [18:01<18:21,  1.11s/it, loss=1.0018]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200, Loss: 1.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  59%|█████▉    | 1301/2198 [19:41<14:39,  1.02it/s, loss=4.5584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300, Loss: 4.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  64%|██████▎   | 1401/2198 [21:21<15:40,  1.18s/it, loss=2.4009]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400, Loss: 2.4009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  68%|██████▊   | 1501/2198 [22:57<11:13,  1.03it/s, loss=1.6814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500, Loss: 1.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  73%|███████▎  | 1601/2198 [24:37<13:36,  1.37s/it, loss=0.3037]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600, Loss: 0.3037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  77%|███████▋  | 1701/2198 [26:14<07:54,  1.05it/s, loss=0.9068]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1700, Loss: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  82%|████████▏ | 1801/2198 [27:51<06:26,  1.03it/s, loss=1.5190]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1800, Loss: 1.5190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  86%|████████▋ | 1901/2198 [29:28<04:47,  1.03it/s, loss=3.5257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1900, Loss: 3.5257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  91%|█████████ | 2001/2198 [31:04<03:05,  1.06it/s, loss=3.6049]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000, Loss: 3.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7:  96%|█████████▌| 2101/2198 [32:39<01:31,  1.06it/s, loss=3.0537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2100, Loss: 3.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/7: 100%|██████████| 2198/2198 [34:11<00:00,  1.07it/s, loss=2.2333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - Avg Train Loss: 2.2278\n",
      "Epoch 3 - Avg Val Loss: 2.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   0%|          | 1/2198 [00:00<35:01,  1.05it/s, loss=4.1606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss: 4.1606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   5%|▍         | 101/2198 [01:38<37:53,  1.08s/it, loss=2.5856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100, Loss: 2.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:   9%|▉         | 201/2198 [03:15<31:45,  1.05it/s, loss=2.3126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200, Loss: 2.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  14%|█▎        | 301/2198 [04:57<30:19,  1.04it/s, loss=2.0419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 300, Loss: 2.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  18%|█▊        | 401/2198 [06:35<30:42,  1.03s/it, loss=1.2103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 400, Loss: 1.2103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  23%|██▎       | 501/2198 [08:15<27:58,  1.01it/s, loss=0.5753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 500, Loss: 0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  27%|██▋       | 601/2198 [09:53<25:38,  1.04it/s, loss=1.1748]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 600, Loss: 1.1748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  32%|███▏      | 701/2198 [11:30<23:53,  1.04it/s, loss=0.9562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 700, Loss: 0.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  36%|███▋      | 801/2198 [13:07<22:32,  1.03it/s, loss=1.2357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 800, Loss: 1.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  41%|████      | 901/2198 [14:46<21:00,  1.03it/s, loss=2.0108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 900, Loss: 2.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  46%|████▌     | 1001/2198 [16:23<19:32,  1.02it/s, loss=2.5192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000, Loss: 2.5192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  50%|█████     | 1101/2198 [18:01<17:52,  1.02it/s, loss=2.6912]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1100, Loss: 2.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  55%|█████▍    | 1201/2198 [19:37<16:09,  1.03it/s, loss=0.0038]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1200, Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  59%|█████▉    | 1301/2198 [21:17<14:25,  1.04it/s, loss=1.6484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1300, Loss: 1.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  64%|██████▎   | 1401/2198 [22:57<12:41,  1.05it/s, loss=2.7655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1400, Loss: 2.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  68%|██████▊   | 1501/2198 [24:44<11:20,  1.02it/s, loss=2.2885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1500, Loss: 2.2885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  73%|███████▎  | 1601/2198 [26:24<10:42,  1.08s/it, loss=1.4723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1600, Loss: 1.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  77%|███████▋  | 1701/2198 [28:04<08:59,  1.09s/it, loss=2.5627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1700, Loss: 2.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  82%|████████▏ | 1801/2198 [29:52<07:04,  1.07s/it, loss=2.6765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1800, Loss: 2.6765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  86%|████████▋ | 1901/2198 [31:44<05:17,  1.07s/it, loss=2.2328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1900, Loss: 2.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  91%|█████████ | 2001/2198 [33:34<04:00,  1.22s/it, loss=3.0487]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000, Loss: 3.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7:  96%|█████████▌| 2101/2198 [35:24<01:39,  1.03s/it, loss=2.2608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2100, Loss: 2.2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/7: 100%|██████████| 2198/2198 [36:58<00:00,  1.01s/it, loss=0.0120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 - Avg Train Loss: 2.0007\n",
      "Epoch 4 - Avg Val Loss: 2.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   0%|          | 1/2198 [00:01<38:20,  1.05s/it, loss=1.6481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0, Loss: 1.6481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   5%|▍         | 101/2198 [01:35<33:08,  1.05it/s, loss=2.0960]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 100, Loss: 2.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:   9%|▉         | 201/2198 [03:40<50:36,  1.52s/it, loss=0.5362]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 200, Loss: 0.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/7:  11%|█         | 239/2198 [04:39<55:34,  1.70s/it, loss=0.9368]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "samplesFiltered = [s for s in samples if len(s[\"target\"].split()) >= 3]\n",
    "\n",
    "# convert existing data to hugging face dataset\n",
    "data = Dataset.from_list(samplesFiltered)\n",
    "\n",
    "# Setup\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def preprocess(samples):\n",
    "    modelInputs = tokenizer(\n",
    "        samples[\"input\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        samples[\"target\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    modelInputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return modelInputs\n",
    "\n",
    "trainValSplit = data.train_test_split(test_size=0.2)\n",
    "valTestSplit = trainValSplit[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "splits = {\n",
    "    \"train\": trainValSplit[\"train\"],\n",
    "    \"validation\": valTestSplit['train'],\n",
    "    \"test\": valTestSplit[\"test\"]\n",
    "}\n",
    "\n",
    "finalData = {\n",
    "    \"train\": splits[\"train\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"validation\": splits[\"validation\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"test\": splits[\"test\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "}\n",
    "\n",
    "dataCollator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    finalData[\"train\"], \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    finalData[\"validation\"],\n",
    "    batch_size=2,\n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "num_epochs = 7\n",
    "device = \"cuda\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # look up more info\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # look up more info\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # look up more info\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"\\nStep {step}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "model.save_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d4eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Answer this question about Kenya: 14.10 Does any law or regulation set...\n",
      "True: Yes, financial ombudsman...\n",
      "Pred: Yes, financial ombudsman\n",
      "\n",
      "Q: Answer this question about Bulgaria: 3.20.3 Are the following items de...\n",
      "True: 100% of T1. If the bank has AT1 - 60% of CET1 and 40% of AT1...\n",
      "Pred: Deducted from CET 1\n",
      "\n",
      "Q: Answer this question about Togo: 3.20.4 Are the following items deduct...\n",
      "True: Pris en compte dans le T1...\n",
      "Pred: La juste valeur nâ€TMest pas transpos dans le corpus rglementaire\n",
      "\n",
      "Q: Answer this question about Taiwan, China: 3.1 Which regulatory capital...\n",
      "True: commercial banks, state-owned commercial banks...\n",
      "Pred: Commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial\n",
      "\n",
      "Q: Answer this question about Bulgaria: 3.20.3 Are the following items de...\n",
      "True: According to applicable thresholds in the CRR and having in ...\n",
      "Pred: Deducted from CET 1\n",
      "\n",
      "Q: Answer this question about Bosnia and Herzegovina: 3.15 What is the re...\n",
      "True: (Tier 1/ total exposure)*100...\n",
      "Pred: Basel III leverage ratio\n",
      "\n",
      "Q: Answer this question about Ukraine: 8.2.1 The insurance fund is manage...\n",
      "True: Deposit Guarantee Fund (hereinafter - DGF) is a legal entity...\n",
      "Pred: Central Bank of Ukraine\n",
      "\n",
      "Q: Answer this question about Ukraine: 3.20.4 Are the following items ded...\n",
      "True: This item (b) provide by Basel III. The NBU plans to impleme...\n",
      "Pred: Deducted from CET 1\n",
      "\n",
      "Q: Answer this question about Thailand: 9.1.1 4 - Description...\n",
      "True: Qualitative factors focus on determining ability to repay de...\n",
      "Pred: â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€\n",
      "\n",
      "Q: Answer this question about Togo: 14.1 What body/agency has the respons...\n",
      "True: Les rÃ©flexions pour la mise en place d'observatoires de la ...\n",
      "Pred: La seule Autorit comptente pour la mise en place d'observatoires de la qualit des services_x000D_ financiers dans les Etats de l'UEH, apr s avis de la Banque Centrale, apr s avis avis de la Banque Centrale, apr s avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis avis\n",
      "\n",
      "Q: Answer this question about Kosovo: 6.1 Have you issued specific guidel...\n",
      "True: Comply or explain...\n",
      "Pred: Comply or explain\n",
      "\n",
      "Q: Answer this question about Croatia: 3.20.1 Which of the following item...\n",
      "True: amortised in last 5 years...\n",
      "Pred: 1.25% of RWA\n",
      "\n",
      "Q: Answer this question about Hungary: 6.1 Have you issued specific guide...\n",
      "True: Comply or explain...\n",
      "Pred: Comply or explain\n",
      "\n",
      "Q: Answer this question about Côte d'Ivoire: 5.5 Can the bank supervisor ...\n",
      "True: Do not know...\n",
      "Pred: Do not know\n",
      "\n",
      "Q: Answer this question about Canada: 3.20.1 Which of the following items...\n",
      "True: For institutions using IRB approach - difference (if positiv...\n",
      "Pred: 1.25% of RWA\n",
      "\n",
      "Q: Answer this question about Denmark: 15.1 Are there any Islamic banks o...\n",
      "True: Do not know...\n",
      "Pred: Do not know\n",
      "\n",
      "Q: Answer this question about Zimbabwe: 3.20.4 Are the following items de...\n",
      "True: Deducted from Tier 1...\n",
      "Pred: Deducted from CET 1\n",
      "\n",
      "Q: Answer this question about Vietnam: 14.5.3 By law or regulation, which...\n",
      "True: Regulations do not specify...\n",
      "Pred: Regulations do not specify\n",
      "\n",
      "Q: Answer this question about Bulgaria: 3.1 Which regulatory capital adeq...\n",
      "True: all banks (Regulation 575/2013 and Directive 2013/36/EU - CR...\n",
      "Pred: Commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial banks, state-owned commercial\n",
      "\n",
      "Q: Answer this question about Israel: 12.24.1 Because you do not have an ...\n",
      "True: Supervised as a bank...\n",
      "Pred: The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry of Finance. The Bank of Israel is supervised by the Ministry\n",
      "custom questions:\n",
      "\n",
      "Q: Answer this question about United States: What body/agency grants banking licenses?\n",
      "A: The National Bank of the United States\n",
      "\n",
      "Q: Answer this question about France: What is the minimum capital requirement?\n",
      "A: The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a bank is £75 million. The minimum capital requirement for a\n",
      "\n",
      "Q: Answer this question about Japan: Who regulates banks?\n",
      "A: Central Bank of Japan\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\")\n",
    "\n",
    "test_indices = random.sample(range(len(data)), 20)\n",
    "\n",
    "for idx in test_indices:\n",
    "    sample = data[idx]\n",
    "    question = sample[\"input\"]\n",
    "    true_answer = sample[\"target\"]\n",
    "    \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    print(f\"\\nQ: {question[:70]}...\")\n",
    "    print(f\"True: {true_answer[:60]}...\")\n",
    "    print(f\"Pred: {predicted}\")\n",
    "\n",
    "# Test on custom questions\n",
    "print(\"custom questions:\")\n",
    "\n",
    "custom_questions = [\n",
    "    \"Answer this question about United States: What body/agency grants banking licenses?\",\n",
    "    \"Answer this question about France: What is the minimum capital requirement?\",\n",
    "    \"Answer this question about Japan: Who regulates banks?\"\n",
    "]\n",
    "\n",
    "for q in custom_questions:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbtraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
