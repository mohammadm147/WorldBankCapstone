{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0f01cb",
   "metadata": {},
   "source": [
    "# World Bank Financial Survey Q&A Model Project \n",
    "\n",
    "This project develops a NLP powererd question-answering system that is trained on World Bank Survey Data containing financial information gathered from various federal banks across the globe. This notebook walks the user through gathering/processing the data and training/deploying the final model. \n",
    "\n",
    "### Dataset Description\n",
    "The World Bank survey dataset comprises of structured financial questions sent to financial instituitions worldwide. The dataset includes multi-dimensional survey responses, hierarchial question structures, and financial metrics. For this project, we will use the questions that have long-form textual answers to train an NLP model, rather than using binary response questions.\n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "##### Phase 1 - Data Processing \n",
    "\n",
    "- Transform unstructured survey data into structurerd NLP training pairs\n",
    "    - Parse all relevant sheets from excel file\n",
    "    - Properly handle hierarchical question structures to ensure each question answer pair is standalone\n",
    "- Indentify and Flag PII using a ML model in dataset\n",
    "\n",
    "##### Phase 2 - Model Development & Fine Tuning\n",
    "\n",
    "- Fine-tune a Google FLAN-T5-Base NLP model \n",
    "- Optimize the model's performance on this specific World Bank survey domain\n",
    "- Evaluate model performance using validation and test sample sets\n",
    "\n",
    "##### Phase 3 - Deployement\n",
    "\n",
    "- Deploy fine-tuned model to production environment on Azure/HuggingFace that allows for web/API interaction\n",
    "\n",
    "##### Future Steps (if time allows):\n",
    "- train FLAN-T5-Base instead of FLAN-T5-Small to improve generalization/accuracy\n",
    "- add some sort of sentiment analysis to clasify questions/answers (financial questions, admin questions, etc)\n",
    "- get feedback on model performance (answer quality/hallucinations/knowledge gaps)  \n",
    "- add additional survey questions to knowledge base \n",
    "\n",
    "##### Model Notes:\n",
    "- The model is trained on a balanced subset that consists of all the long form questions in the dataset and a random 30% selection of classification/numerical questions.\n",
    "    - This was done due to time constraints. Training on the entire dataset would have taken multiple days\n",
    "    - Additionally, the dataset is severely imbalanced. I found that training with a high number of classification samples meant the model would learn to answer yes or 0 for each one\n",
    "- Additionally, the model being trained is FLAN-T5-SMALL rather than FLAN-T5-BASE\n",
    "    - SMALL is significantly smaller than BASE (60M vs 220M parameters)\n",
    "    - The smaller model was chosen due to hardware limitations and time constraints\n",
    "    - This means that this model's accuracy and ability to generalize isn't as good as it could be\n",
    "    - In future versions, given enough time, I would have liked to implement the BASE model instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfe9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from World Bank Database\n",
    "import requests\n",
    "\n",
    "url = \"https://datacatalogfiles.worldbank.org/ddh-published/0038632/2/DR0047737/2021_04_26_brss-public-release.xlsx\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"worldbank_data.xlsx\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2654b84",
   "metadata": {},
   "source": [
    "Now that data is downloaded, it needs to be converted from an xlsx file with row column format to something that works for t5 training (question:answer pairs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea782ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read and process data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# remove extra unnecessary information from question\n",
    "# for example, \"Select all that apply\"\n",
    "def simplify_question(qText):\n",
    "    if pd.isna(qText):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(qText).strip()\n",
    "    \n",
    "    # split on common instruction starters and take first part\n",
    "    for splitter in [\" Please \", \" If \", \" Include \", \" Specify \", \" Describe \", \" List \"]:\n",
    "        if splitter in text:\n",
    "            text = text.split(splitter)[0]\n",
    "            break\n",
    "    \n",
    "    # if there's a question mark, take up to first one\n",
    "    if \"?\" in text:\n",
    "        text = text.split(\"?\")[0] + \"?\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# loads all sheets at once\n",
    "allSheets = pd.ExcelFile(\"worldbank_data.xlsx\")\n",
    "\n",
    "# store samples\n",
    "samples = []\n",
    "\n",
    "# process all sheets except first 2 and last 1\n",
    "process = allSheets.sheet_names[2:-1]\n",
    "\n",
    "# read first sheet and extract countries\n",
    "dfFirst = pd.read_excel(allSheets, sheet_name=process[0], header=None)\n",
    "countries = [str(c) for c in dfFirst.iloc[0, 2:].values if not pd.isna(c)]\n",
    "\n",
    "for sheet in process:\n",
    "    # read current sheet\n",
    "    df = pd.read_excel(allSheets, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # create parent and base vars\n",
    "    parent = None\n",
    "    currBase = None\n",
    "    \n",
    "    # iterate through every row except header\n",
    "    # get question index and question text\n",
    "    for idx, row in df.iloc[1:].iterrows():\n",
    "        qIndex = row[0]\n",
    "        qText = row[1]\n",
    "        \n",
    "        # if the question index is null but text does exist \n",
    "        # then the question is a parent question\n",
    "        # assign parent question and then clear prev base and move onto next row\n",
    "        if pd.isna(qIndex) and not pd.isna(qText):\n",
    "            parent = simplify_question(qText)  # ← Simplify parent too\n",
    "            currBase = None\n",
    "            continue\n",
    "        \n",
    "        # regex starts with Q and captures groups delimited by _\n",
    "        # group 1 is the main question number\n",
    "        # group 2 is sub-question number\n",
    "        # group 3 is for multi-part questions with extra text\n",
    "        # non-capturing group is for sections of index which are unnecessary\n",
    "        match = re.match(r'Q(\\d+)_([0-9_]+?)([a-zA-Z_]+)?(?:_[A-Z]|_\\d{4}|$)', str(qIndex))\n",
    "        \n",
    "        # if regex matched then process row, otherwise skip\n",
    "        if match:\n",
    "            baseNum = f\"{match.group(1)}_{match.group(2)}\"\n",
    "            isMulti = bool(match.group(3)) or bool(re.search(r'_\\d{4}', str(qIndex)))\n",
    "            part = match.group(3) if match.group(3) else \"\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # if new base is different to current base, update base\n",
    "        if baseNum and baseNum != currBase:\n",
    "            # reset parent if new question isn't multi part\n",
    "            if not isMulti:\n",
    "                parent = None\n",
    "            currBase = baseNum\n",
    "        \n",
    "        # loop through each column\n",
    "        for colIdx, country in enumerate(countries):\n",
    "            \n",
    "            # get answer for current column\n",
    "            answer = row[colIdx + 2]\n",
    "            \n",
    "            # skip column if there's no answer\n",
    "            if pd.isna(answer):\n",
    "                continue\n",
    "            \n",
    "            # Simplify the question text\n",
    "            simplifiedQ = simplify_question(qText)  # ← KEY CHANGE\n",
    "            \n",
    "            # if question is multi-part combine parent question and question text\n",
    "            if isMulti and parent:\n",
    "                completeQ = f\"{parent} {simplifiedQ}\"\n",
    "            # otherwise just append question text\n",
    "            else:\n",
    "                completeQ = simplifiedQ\n",
    "            \n",
    "            # fill in sample entry\n",
    "            sample = {\n",
    "                \"input\": f\"Answer this question about {country}: {completeQ}\".strip(),\n",
    "                \"target\": str(answer).strip()\n",
    "            }\n",
    "            \n",
    "            # append sample to list\n",
    "            samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920245f",
   "metadata": {},
   "source": [
    "Now that the data is in proper training format, it needs to be checked for PII. We will use Microsoft's Presidio pre-trained ML library to detect PII (https://github.com/microsoft/presidio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependecies\n",
    "# !pip install presidio_analyzer presidio_anonymizer\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb74525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# initialize analyzer\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# specific countries and years are necessary to the survey data\n",
    "# do not flag these as PII\n",
    "excludeWords = set(countries)\n",
    "excludeWords.update(['2011', '2012', '2013', '2014', '2015', '2016'])\n",
    "\n",
    "# only include entries that the model has 70%+ confidnece is PII\n",
    "CONFIDENCE = 0.7\n",
    "\n",
    "# only track unique PII values\n",
    "seenPII = set()\n",
    "\n",
    "# storage for PII\n",
    "potentialPII = []\n",
    "\n",
    "# iterate through every sample\n",
    "for idx, sample in enumerate(tqdm(samples, desc='finding pii')):\n",
    "\n",
    "    # get input question and target\n",
    "    inputText = sample[\"input\"]\n",
    "    targetText = sample[\"target\"]\n",
    "\n",
    "    # analyze input and target\n",
    "    inputRes = analyzer.analyze(text=inputText, language='en')\n",
    "    targetRes = analyzer.analyze(text=targetText, language='en')\n",
    "\n",
    "    # filter out exclude list from text matches\n",
    "    inputRes = [r for r in inputRes \n",
    "                if r.score >= CONFIDENCE\n",
    "                and not any(inputText[r.start:r.end] in word or word in inputText[r.start:r.end] for word in excludeWords)] \n",
    "    targetRes = [r for r in targetRes \n",
    "                 if r.score >= CONFIDENCE \n",
    "                 and not any(targetText[r.start:r.end] in word or word in targetText[r.start:r.end] for word in excludeWords)]\n",
    "\n",
    "    # if pii is found\n",
    "    isNewPII = False\n",
    "    for r in inputRes:\n",
    "        if inputText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(inputText[r.start:r.end])\n",
    "    for r in targetRes:\n",
    "        if targetText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(targetText[r.start:r.end])\n",
    "\n",
    "    if isNewPII:\n",
    "        res = {\n",
    "            \"input\": inputText,\n",
    "            \"target\": targetText,\n",
    "            \"inputPII\": [{\"type\": r.entity_type, \"text\": inputText[r.start:r.end], \"score\": r.score} for r in inputRes],\n",
    "            \"targetPII\": [{\"type\": r.entity_type, \"text\": targetText[r.start:r.end], \"score\": r.score} for r in targetRes]\n",
    "        }\n",
    "        potentialPII.append(res)\n",
    "\n",
    "# dump all potential flagged PII into a json file\n",
    "with open('potentialPII.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(potentialPII, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa1d82",
   "metadata": {},
   "source": [
    "The code dumps all potential PII matches to a seperate JSON file saved to the current directory (potentiallyPII.json). This file can now be manually checked to determine which flagged keywords are false postives and which are actually PII. Once all PII is removed from the dataset, the T5 model training can begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# balances samples to significantly reduce training time for project constraints\n",
    "# also helps prevent the model from learning to predict yes/no for every question\n",
    "samplesSmall = [s for s in samples if len(s[\"target\"].split()) < 3]\n",
    "samplesLarge = [s for s in samples if len(s[\"target\"].split()) >= 3]\n",
    "random.seed(42)\n",
    "samplesBalanced = (\n",
    "    random.sample(samplesLarge, min(int(len(samples) * 0.7), len(samplesLarge))) + \n",
    "    random.sample(samplesSmall, min(int(len(samples) * 0.3), len(samplesSmall)))\n",
    ")\n",
    "random.shuffle(samplesBalanced)\n",
    "\n",
    "# convert existing data to hugging face dataset\n",
    "data = Dataset.from_list(samplesBalanced)\n",
    "\n",
    "# setup base model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# tokenize inputs and targets\n",
    "def preprocess(samples):\n",
    "    modelInputs = tokenizer(\n",
    "        samples[\"input\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        samples[\"target\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    modelInputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return modelInputs\n",
    "\n",
    "# split data for testing and validation\n",
    "trainValSplit = data.train_test_split(test_size=0.2)\n",
    "valTestSplit = trainValSplit[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "splits = {\n",
    "    \"train\": trainValSplit[\"train\"],\n",
    "    \"validation\": valTestSplit['train'],\n",
    "    \"test\": valTestSplit[\"test\"]\n",
    "}\n",
    "\n",
    "finalData = {\n",
    "    \"train\": splits[\"train\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"validation\": splits[\"validation\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"test\": splits[\"test\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "}\n",
    "\n",
    "# create data collator\n",
    "dataCollator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# create adam optimizer with loss function, learning rate, and weight decay params\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# learning rate sceduling - reduces the lr over all epochs\n",
    "# helps model converge better by reducing oscillation\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=7)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    finalData[\"train\"], \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    finalData[\"validation\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "# define epochs and specify gpu for training\n",
    "num_epochs = 7\n",
    "device = \"cuda\"\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # input_ids is the tokenized input\n",
    "        # attention_mask tells the model which tokens are important (padding vs content)\n",
    "        # labels are the tokenized targets\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # forward pass - feeding current batch to model \n",
    "        # logits are raw, unnormalized scores - can be considered the model's \"thoughts\"\n",
    "        # loss function includes label_smoothing - makes the model less confident and improves generalization (ability to perform on unseen data)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = CrossEntropyLoss(label_smoothing=0.1, ignore_index=-100)\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))        \n",
    "        \n",
    "        # sets gradients back to zero so they are fresh for new batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # computes the gradient of the loss of all the weights and biases\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping - prevents gradients from breaking if model updates by large amount\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # updates model's parameters\n",
    "        # Adam optimizer uses the gradients calculated by loss.backward() to make adjustments\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss and progress bar\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # defines how big the next lr step should be - learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # validation testing\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    # validates model on samples without modifying weights\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "# save model once finetuned\n",
    "model.save_pretrained(\"./flan-t5-small-label-smooth-balanced\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-small-label-smooth-balanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dc213",
   "metadata": {},
   "source": [
    "Now that the model is fine-tuned on the initial dataset, it can be locally queried to it correctly provides predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-t5-small-label-smooth-balanced\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./flan-t5-small-label-smooth-balanced\")\n",
    "\n",
    "# select random test samples\n",
    "test_indices = random.sample(range(len(data)), 20)\n",
    "\n",
    "for idx in test_indices:\n",
    "    sample = data[idx]\n",
    "    question = sample[\"input\"]\n",
    "    true_answer = sample[\"target\"]\n",
    "    \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    print(f\"\\nQ: {question[:70]}...\")\n",
    "    print(f\"True: {true_answer[:60]}...\")\n",
    "    print(f\"Pred: {predicted}\")\n",
    "\n",
    "# test on custom questions\n",
    "print(\"\\ncustom questions:\")\n",
    "custom_questions = [\n",
    "    \"Answer this question about United States: What body/agency grants banking licenses?\",\n",
    "    \"Answer this question about France: What is the minimum capital requirement?\",\n",
    "    \"Answer this question about Japan: Who regulates banks?\"\n",
    "]\n",
    "\n",
    "for q in custom_questions:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cb337",
   "metadata": {},
   "source": [
    "Now that we have made sure the model is working, we can upload it to a server. In this project, I'm using HuggingFace as its free and allows for easy testing. For production we would use Azure/AWS/GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in to huggingface\n",
    "# you may need to run the authentication command directly in your terminal \n",
    "!pip install huggingface_hub\n",
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b285a70",
   "metadata": {},
   "source": [
    "Now that you are logged in to huggingface, you must upload the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model to hf:\n",
    "model.push_to_hub(\"mian21/flan-t5-small-label-smooth-balanced\")\n",
    "tokenizer.push_to_hub(\"mian21/flan-t5-small-label-smooth-balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844d705",
   "metadata": {},
   "source": [
    "The model is now available from huggingface's website and can be loaded directly into your code using huggingface's autotrainer.\n",
    "\n",
    "Additionally, the model can be viewed online through the huggingface model page or interacted with directly through the huggingface space.\n",
    "\n",
    "https://huggingface.co/mian21/flan-t5-small-label-smooth-balanced  \n",
    "https://huggingface.co/spaces/mian21/t5-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e46298",
   "metadata": {},
   "source": [
    "The model can also be queried through API calls when deployed on a cloud server like Google Cloud Platform, Amazon Web Services, or Microsft Azure. In this project, I will be using Azure to demostrate endpoint deployement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d665f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install azure dependencies and login to azure\n",
    "!pip install azure-ai-ml azure-cli azure-identity\n",
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba643e60",
   "metadata": {},
   "source": [
    "Now that you are logging into Azure, create the resource group and workspace for the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b24799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Workspace\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "import time\n",
    "\n",
    "# define azure values\n",
    "SUBSCRIPTION_ID = \"a5937ed9-afe1-4645-9cf0-7e50f1e2b2d3\"\n",
    "RESOURCE_GROUP = \"t5-model-deployment\"\n",
    "WORKSPACE_NAME = \"t5-WB-workspace\"\n",
    "LOCATION = \"eastus\"\n",
    "\n",
    "# authenticate\n",
    "creds = DefaultAzureCredential()\n",
    "\n",
    "# create resource client\n",
    "resourceClient = ResourceManagementClient(creds, SUBSCRIPTION_ID)\n",
    "\n",
    "# create resource group\n",
    "rgRes = resourceClient.resource_groups.create_or_update(\n",
    "    RESOURCE_GROUP,\n",
    "    {\"location\": LOCATION}\n",
    ")\n",
    "\n",
    "# create workspace\n",
    "mlClient = MLClient(creds, SUBSCRIPTION_ID, RESOURCE_GROUP)\n",
    "workspace = Workspace(\n",
    "    name=WORKSPACE_NAME,\n",
    "    location=LOCATION,\n",
    "    description=\"deployment workspace for WB fine-tuned t5 model\"\n",
    ")\n",
    "workspace = mlClient.workspaces.begin_create(workspace).result()\n",
    "\n",
    "print(\"azure deployment details:\")\n",
    "print(f\"\\nsubscription id: {SUBSCRIPTION_ID}\")\n",
    "print(f\"\\nresource group: {RESOURCE_GROUP}\")\n",
    "print(f\"\\nworkspace name: {WORKSPACE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312f2a5",
   "metadata": {},
   "source": [
    "Azure requires a scoring script that defines how to use the model to make predictions and a conda environment file that lists the endpoint requirements. Before we upload the model, we need to create these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79180a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make a new directory for azure files\n",
    "os.makedirs(\"azure\", exist_ok=True)\n",
    "\n",
    "scoringScript = \"\"\"import os, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def init():\n",
    "    global model, tokenizer\n",
    "\n",
    "    # define model/tokenizer path\n",
    "    modelDir = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"flan-t5-small-label-smooth-balanced\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(modelDir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelDir)\n",
    "    model.eval()\n",
    "\n",
    "def run(rawData):\n",
    "    try:\n",
    "        # load data from parameter and get question\n",
    "        data = json.loads(rawData)\n",
    "        inputs = data[\"inputs\"]\n",
    "\n",
    "        # tokenize input\n",
    "        encoded = tokenizer(\n",
    "            inputs,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # generate output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return json.dumps({\"predictions\": predictions})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\"\"\"\n",
    "\n",
    "# write scoring file to azure directory\n",
    "with open(\"azure/score.py\", \"w\") as f:\n",
    "    f.write(scoringScript)\n",
    "\n",
    "condaENV = \"\"\"name: model-env\n",
    "channels:\n",
    "    - conda-forge\n",
    "    - defaults\n",
    "dependencies:\n",
    "    - python=3.8\n",
    "    - pip\n",
    "    - pip:\n",
    "        - azureml-defaults\n",
    "        - transformers==4.44.0\n",
    "        - torch==2.1.0\n",
    "        - inference-schema\n",
    "        - sentencepiece\n",
    "        - protobuf\n",
    "        - tokenizers==0.19.1\n",
    "\"\"\"\n",
    "\n",
    "# write conda file\n",
    "with open(\"azure/conda.yaml\", \"w\") as f:\n",
    "    f.write(condaENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bd627",
   "metadata": {},
   "source": [
    "Before we create the actual endpoint itself, we must register the necessary resource providers for ML endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import time\n",
    "\n",
    "SUBSCRIPTION_ID = \"a5937ed9-afe1-4645-9cf0-7e50f1e2b2d3\"\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "resource_client = ResourceManagementClient(credential, SUBSCRIPTION_ID)\n",
    "\n",
    "# list of all providers needed for ML endpoints\n",
    "providersToRegister = [\n",
    "    'Microsoft.MachineLearningServices',\n",
    "    'Microsoft.ContainerInstance',\n",
    "    'Microsoft.Storage',\n",
    "    'Microsoft.KeyVault',\n",
    "    'Microsoft.ContainerRegistry',\n",
    "    'Microsoft.Insights',\n",
    "    'Microsoft.Compute',\n",
    "    'Microsoft.Network',\n",
    "    'Microsoft.Cdn',\n",
    "    'Microsoft.PolicyInsights'\n",
    "]\n",
    "\n",
    "for providerName in providersToRegister:\n",
    "    provider = resource_client.providers.register(providerName)\n",
    "    \n",
    "    # wait for this provider to finish registering\n",
    "    while provider.registration_state == 'Registering':\n",
    "        time.sleep(20)\n",
    "        provider = resource_client.providers.get(providerName)\n",
    "    \n",
    "    print(f\"{providerName}: {provider.registration_state}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa1d1b",
   "metadata": {},
   "source": [
    "Now that our environment is setup, we can deploy the model to Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# azure values we created earlier\n",
    "SUBSCRIPTION_ID = \"a5937ed9-afe1-4645-9cf0-7e50f1e2b2d3\"\n",
    "RESOURCE_GROUP = \"t5-model-deployment\"\n",
    "WORKSPACE_NAME = \"t5-WB-workspace\"\n",
    "\n",
    "# connect to workspace\n",
    "mlClient = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME\n",
    ")\n",
    "\n",
    "# register the model\n",
    "model = Model(\n",
    "    path=\"./flan-t5-small-label-smooth-balanced\",\n",
    "    name=\"flan-t5-WB-finetuned\",\n",
    "    description=\"T5 model fine tuned on World Bank Survey Data\"\n",
    ")\n",
    "registeredModel = mlClient.models.create_or_update(model)\n",
    "print(f\"model registered: {registeredModel.name}\")\n",
    "\n",
    "# create endpoint\n",
    "endpointName = \"t5-endpoint\"\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpointName,\n",
    "    description=\"Endpoint for fine-tuned FLAN-T5\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "mlClient.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(f\"endpoint created: {endpointName}\")\n",
    "\n",
    "# create deployment\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    # azure uses blue/green naming conventions\n",
    "    # blue represents production endpoint while green is for testing\n",
    "    name=\"blue\",\n",
    "    endpoint_name=endpointName,\n",
    "    model=registeredModel.id,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./azure\",\n",
    "        scoring_script=\"score.py\"\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        conda_file=\"./azure/conda.yaml\",\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04\"\n",
    "    ),\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1\n",
    ")\n",
    "mlClient.online_deployments.begin_create_or_update(deployment).result()\n",
    "print(\"deployment created\")\n",
    "\n",
    "# direct traffic to deployment\n",
    "# we currently don't have a testing version as green so direct all traffic to blue/production\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "mlClient.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(\"traffic is configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65392d",
   "metadata": {},
   "source": [
    "Now that the endpoint is configured and running. It can be used to query the model and receive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9860415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# connect to workspace\n",
    "mlClient = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=\"a5937ed9-afe1-4645-9cf0-7e50f1e2b2d3\",\n",
    "    resource_group_name=\"t5-model-deployment\",\n",
    "    workspace_name=\"t5-WB-workspace\"\n",
    ")\n",
    "\n",
    "# get endpoint name\n",
    "endpointName = \"t5-endpoint\"\n",
    "endpoint = mlClient.online_endpoints.get(name=endpointName)\n",
    "\n",
    "# Get the scoring URI and authentication key\n",
    "scoringUri = endpoint.scoring_uri\n",
    "keys = mlClient.online_endpoints.get_keys(name=endpointName)\n",
    "authKey = keys.primary_key\n",
    "\n",
    "print(f\"Scoring URI: {scoringUri}\")\n",
    "print(f\"Auth Key: {authKey[:10]}...\")\n",
    "\n",
    "# prepare questions\n",
    "questions = {\n",
    "    \"inputs\": [\n",
    "        \"What body/agency grants banking licenses in the United States?\",\n",
    "        \"What is the minimum capital requirement in France?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send request to endpoint\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {authKey}\"\n",
    "}\n",
    "\n",
    "response = requests.post(scoringUri, headers=headers, data=json.dumps(questions))\n",
    "\n",
    "# print response\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbtraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
