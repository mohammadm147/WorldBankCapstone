{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0f01cb",
   "metadata": {},
   "source": [
    "# World Bank Financial Survey Q&A Model Project \n",
    "\n",
    "This project develops a NLP powererd question-answering system that is trained on World Bank Survey Data containing financial information gathered from various federal banks across the globe. This notebook walks the user through gathering/processing the data and training/deploying the final model. \n",
    "\n",
    "### Dataset Description\n",
    "The World Bank survey dataset comprises of structured financial questions sent to financial instituitions worldwide. The dataset includes multi-dimensional survey responses, hierarchial question structures, and financial metrics. For this project, we will use the questions that have long-form textual answers to train an NLP model, rather than using binary response questions.\n",
    "\n",
    "### Project Architecture\n",
    "\n",
    "##### Phase 1 - Data Processing \n",
    "\n",
    "- Transform unstructured survey data into structurerd NLP training pairs\n",
    "    - Parse all relevant sheets from excel file\n",
    "    - Properly handle hierarchical question structures to ensure each question answer pair is standalone\n",
    "- Indentify and Flag PII using a ML model in dataset\n",
    "\n",
    "##### Phase 2 - Model Development & Fine Tuning\n",
    "\n",
    "- Fine-tune a Google FLAN-T5-Base NLP model \n",
    "- Optimize the model's performance on this specific World Bank survey domain\n",
    "- Evaluate model performance using validation and test sample sets\n",
    "\n",
    "##### Phase 3 - Deployement\n",
    "\n",
    "- Deploy fine-tuned model to production environment on Azure/Huggingface\n",
    "\n",
    "##### Future Steps (if time allows):\n",
    "- implement API for interacting with model\n",
    "- add some sort of sentiment analysis to clasify questions/answers (financial questions, admin questions, etc)\n",
    "- get feedback on model performance (answer quality/hallucinations/knowledge gaps)  \n",
    "- add additional survey questions to knowledge base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfe9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from World Bank Database\n",
    "import requests\n",
    "\n",
    "url = \"https://datacatalogfiles.worldbank.org/ddh-published/0038632/2/DR0047737/2021_04_26_brss-public-release.xlsx\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"worldbank_data.xlsx\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2654b84",
   "metadata": {},
   "source": [
    "Now that data is downloaded, it needs to be converted from an xlsx file with row column format to something that works for t5 training (question:answer pairs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea782ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read and process data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Remove extra unnecessary information from question\n",
    "# For example, \"Select all that apply\"\n",
    "def simplify_question(qText):\n",
    "    if pd.isna(qText):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(qText).strip()\n",
    "    \n",
    "    # split on common instruction starters and take first part\n",
    "    for splitter in [\" Please \", \" If \", \" Include \", \" Specify \", \" Describe \", \" List \"]:\n",
    "        if splitter in text:\n",
    "            text = text.split(splitter)[0]\n",
    "            break\n",
    "    \n",
    "    # if there's a question mark, take up to first one\n",
    "    if \"?\" in text:\n",
    "        text = text.split(\"?\")[0] + \"?\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# loads all sheets at once\n",
    "allSheets = pd.ExcelFile(\"worldbank_data.xlsx\")\n",
    "\n",
    "# store samples\n",
    "samples = []\n",
    "\n",
    "# process all sheets except first 2 and last 1\n",
    "process = allSheets.sheet_names[2:-1]\n",
    "\n",
    "# read first sheet and extract countries\n",
    "dfFirst = pd.read_excel(allSheets, sheet_name=process[0], header=None)\n",
    "countries = [str(c) for c in dfFirst.iloc[0, 2:].values if not pd.isna(c)]\n",
    "\n",
    "for sheet in process:\n",
    "    # read current sheet\n",
    "    df = pd.read_excel(allSheets, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # create parent and base vars\n",
    "    parent = None\n",
    "    currBase = None\n",
    "    \n",
    "    # iterate through every row except header\n",
    "    # get question index and question text\n",
    "    for idx, row in df.iloc[1:].iterrows():\n",
    "        qIndex = row[0]\n",
    "        qText = row[1]\n",
    "        \n",
    "        # if the question index is null but text does exist \n",
    "        # then the question is a parent question\n",
    "        # assign parent question and then clear prev base and move onto next row\n",
    "        if pd.isna(qIndex) and not pd.isna(qText):\n",
    "            parent = simplify_question(qText)  # ← Simplify parent too\n",
    "            currBase = None\n",
    "            continue\n",
    "        \n",
    "        # regex starts with Q and captures groups delimited by _\n",
    "        # group 1 is the main question number\n",
    "        # group 2 is sub-question number\n",
    "        # group 3 is for multi-part questions with extra text\n",
    "        # non-capturing group is for sections of index which are unnecessary\n",
    "        match = re.match(r'Q(\\d+)_([0-9_]+?)([a-zA-Z_]+)?(?:_[A-Z]|_\\d{4}|$)', str(qIndex))\n",
    "        \n",
    "        # if regex matched then process row, otherwise skip\n",
    "        if match:\n",
    "            baseNum = f\"{match.group(1)}_{match.group(2)}\"\n",
    "            isMulti = bool(match.group(3)) or bool(re.search(r'_\\d{4}', str(qIndex)))\n",
    "            part = match.group(3) if match.group(3) else \"\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # if new base is different to current base, update base\n",
    "        if baseNum and baseNum != currBase:\n",
    "            # reset parent if new question isn't multi part\n",
    "            if not isMulti:\n",
    "                parent = None\n",
    "            currBase = baseNum\n",
    "        \n",
    "        # loop through each column\n",
    "        for colIdx, country in enumerate(countries):\n",
    "            \n",
    "            # get answer for current column\n",
    "            answer = row[colIdx + 2]\n",
    "            \n",
    "            # skip column if there's no answer\n",
    "            if pd.isna(answer):\n",
    "                continue\n",
    "            \n",
    "            # Simplify the question text\n",
    "            simplifiedQ = simplify_question(qText)  # ← KEY CHANGE\n",
    "            \n",
    "            # if question is multi-part combine parent question and question text\n",
    "            if isMulti and parent:\n",
    "                completeQ = f\"{parent} {simplifiedQ}\"\n",
    "            # otherwise just append question text\n",
    "            else:\n",
    "                completeQ = simplifiedQ\n",
    "            \n",
    "            # fill in sample entry\n",
    "            sample = {\n",
    "                \"input\": f\"Answer this question about {country}: {completeQ}\".strip(),\n",
    "                \"target\": str(answer).strip()\n",
    "            }\n",
    "            \n",
    "            # append sample to list\n",
    "            samples.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920245f",
   "metadata": {},
   "source": [
    "Now that the data is in proper training format, it needs to be checked for PII. We will use Microsoft's Presidio pre-trained ML library to detect PII (https://github.com/microsoft/presidio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependecies\n",
    "# !pip install presidio_analyzer presidio_anonymizer\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb74525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "finding pii: 100%|██████████| 107833/107833 [35:25<00:00, 50.73it/s] \n"
     ]
    }
   ],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# initialize analyzer\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# specific countries and years are necessary to the survey data\n",
    "# do not flag these as PII\n",
    "excludeWords = set(countries)\n",
    "excludeWords.update(['2011', '2012', '2013', '2014', '2015', '2016'])\n",
    "\n",
    "# only include entries that the model has 70%+ confidnece is PII\n",
    "CONFIDENCE = 0.7\n",
    "\n",
    "# only track unique PII values\n",
    "seenPII = set()\n",
    "\n",
    "# storage for PII\n",
    "potentialPII = []\n",
    "\n",
    "# iterate through every sample\n",
    "for idx, sample in enumerate(tqdm(samples, desc='finding pii')):\n",
    "\n",
    "    # get input question and target\n",
    "    inputText = sample[\"input\"]\n",
    "    targetText = sample[\"target\"]\n",
    "\n",
    "    # analyze input and target\n",
    "    inputRes = analyzer.analyze(text=inputText, language='en')\n",
    "    targetRes = analyzer.analyze(text=targetText, language='en')\n",
    "\n",
    "    # filter out exclude list from text matches\n",
    "    inputRes = [r for r in inputRes \n",
    "                if r.score >= CONFIDENCE\n",
    "                and not any(inputText[r.start:r.end] in word or word in inputText[r.start:r.end] for word in excludeWords)] \n",
    "    targetRes = [r for r in targetRes \n",
    "                 if r.score >= CONFIDENCE \n",
    "                 and not any(targetText[r.start:r.end] in word or word in targetText[r.start:r.end] for word in excludeWords)]\n",
    "\n",
    "    # if pii is found\n",
    "    isNewPII = False\n",
    "    for r in inputRes:\n",
    "        if inputText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(inputText[r.start:r.end])\n",
    "    for r in targetRes:\n",
    "        if targetText[r.start:r.end] not in seenPII:\n",
    "            isNewPII = True\n",
    "            seenPII.add(targetText[r.start:r.end])\n",
    "\n",
    "    if isNewPII:\n",
    "        res = {\n",
    "            \"input\": inputText,\n",
    "            \"target\": targetText,\n",
    "            \"inputPII\": [{\"type\": r.entity_type, \"text\": inputText[r.start:r.end], \"score\": r.score} for r in inputRes],\n",
    "            \"targetPII\": [{\"type\": r.entity_type, \"text\": targetText[r.start:r.end], \"score\": r.score} for r in targetRes]\n",
    "        }\n",
    "        potentialPII.append(res)\n",
    "\n",
    "# dump all potential flagged PII into a json file\n",
    "with open('potentialPII.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(potentialPII, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa1d82",
   "metadata": {},
   "source": [
    "The code dumps all potential PII matches to a seperate JSON file saved to the current directory (potentiallyPII.json). This file can now be manually checked to determine which flagged keywords are false postives and which are actually PII. Once all PII is removed from the dataset, the T5 model training can begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30276/30276 [00:03<00:00, 8391.21 examples/s] \n",
      "Map: 100%|██████████| 3784/3784 [00:00<00:00, 16175.57 examples/s]\n",
      "Map: 100%|██████████| 3785/3785 [00:00<00:00, 14886.58 examples/s]\n",
      "Epoch 1/7:  15%|█▍        | 1114/7569 [03:43<18:23,  5.85it/s, loss=2.7522] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "# balances samples to significantly reduce training time for project constraints\n",
    "# also helps prevent the model from learning to predict yes/no for every question\n",
    "samplesSmall = [s for s in samples if len(s[\"target\"].split()) < 3]\n",
    "samplesLarge = [s for s in samples if len(s[\"target\"].split()) >= 3]\n",
    "random.seed(42)\n",
    "samplesBalanced = (\n",
    "    random.sample(samplesLarge, min(int(len(samples) * 0.7), len(samplesLarge))) + \n",
    "    random.sample(samplesSmall, min(int(len(samples) * 0.3), len(samplesSmall)))\n",
    ")\n",
    "random.shuffle(samplesBalanced)\n",
    "\n",
    "# convert existing data to hugging face dataset\n",
    "data = Dataset.from_list(samplesBalanced)\n",
    "\n",
    "# Setup\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def preprocess(samples):\n",
    "    modelInputs = tokenizer(\n",
    "        samples[\"input\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        samples[\"target\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    modelInputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return modelInputs\n",
    "\n",
    "trainValSplit = data.train_test_split(test_size=0.2)\n",
    "valTestSplit = trainValSplit[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "splits = {\n",
    "    \"train\": trainValSplit[\"train\"],\n",
    "    \"validation\": valTestSplit['train'],\n",
    "    \"test\": valTestSplit[\"test\"]\n",
    "}\n",
    "\n",
    "finalData = {\n",
    "    \"train\": splits[\"train\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"validation\": splits[\"validation\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"]),\n",
    "    \"test\": splits[\"test\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "}\n",
    "\n",
    "dataCollator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
    "\n",
    "# learning rate sceduling - reduces the lr over all epochs\n",
    "# helps model converge better by reducing oscillation\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=7)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    finalData[\"train\"], \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    finalData[\"validation\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=dataCollator\n",
    ")\n",
    "\n",
    "num_epochs = 7\n",
    "device = \"cuda\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # look up more info\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # look up more info\n",
    "        # added label_smoothing - makes the model less confident and improves generalization (ability to perform in unseen data)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = CrossEntropyLoss(label_smoothing=0.1, ignore_index=-100)\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))        \n",
    "        \n",
    "        # look up more info\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping - prevents gradients from breaking if model updates by large amount\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"\\nEpoch {epoch+1} - Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "model.save_pretrained(\"./flan-t5-small-label-smooth-balanced\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-small-label-smooth-balanced\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732dc213",
   "metadata": {},
   "source": [
    "Now that the model is fine-tuned on the initial dataset, it can be locally queried to it correctly provides predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d4eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Answer this question about Portugal: 14.1 What body/agency has the res...\n",
      "True: The Central Bank for the retail banking sector. ComissÃ£o do...\n",
      "Pred: The Financial Supervisory Authority of Portugal (Financial Supervisory Authority) is responsible for implementing, overseeing and enforcing any aspects of financial consumer protection laws and regulations. The Financial Supervisory Authority of Portugal (Financial Supervisory Authority) is responsible for implementing, overseeing and enforcing any aspects of financial consumer protection laws and regulations.\n",
      "\n",
      "Q: Answer this question about Cayman Islands: 3.20.1 Which of the followi...\n",
      "True: Limit of 1.25% of risk weighted assets...\n",
      "Pred: Tier 2 capital is not recognised by the Cayman Islands Monetary Authority\n",
      "\n",
      "Q: Answer this question about Lebanon: 12.1.1 a. Commercial banks...\n",
      "True: Banking Control Commision of Lebanon and Special investigati...\n",
      "Pred: Banking Control Commision of Lebanon and Special investigation Committee\n",
      "\n",
      "Q: Answer this question about Panama: 3.20.3 Are the following items dedu...\n",
      "True: No regulation yet...\n",
      "Pred: There are no deductions on Tier 1\n",
      "\n",
      "Q: Answer this question about Finland: 3.20.1 Which of the following item...\n",
      "True: Provided the instrument fulfils the CRR requirements for T2 ...\n",
      "Pred: Fund for general banking risk is allowed in CET1 capital (common equity tier 1 capital). Fund for general banking risk is allowed in CET1 capital (common equity tier 1 capital). Fund for general banking risk is allowed in CET1 capital (common equity tier 1 capital).\n",
      "\n",
      "Q: Answer this question about Guatemala: 9.1.1 5 - Type...\n",
      "True: High risk of irrecoverability...\n",
      "Pred: Crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pago hist3ricos, crditos de pa\n",
      "\n",
      "Q: Answer this question about San Marino: 8.2.1 The insurance fund is man...\n",
      "True: Central Bank of the Republic of San Marino...\n",
      "Pred: The fund is managed by the Central Bank of the Republic of San Marino.\n",
      "\n",
      "Q: Answer this question about Finland: 3.20.4 Are the following items ded...\n",
      "True: Deducted from CET1...\n",
      "Pred: Deducted from CET1\n",
      "\n",
      "Q: Answer this question about Slovenia: 14.3 What actions can the consume...\n",
      "True: Issue order to rectify breaches by a bank, issue order requi...\n",
      "Pred: The Slovenian Consumer Ombudsman may issue administrative sanctions to the banks for infringing the consumer protection laws and regulations. These sanctions may include reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs customers, reprimands of the bankâ€TMs\n",
      "\n",
      "Q: Answer this question about Russian Federation: 12.1.1 d. Bank-holding ...\n",
      "True: The Bank of Russia has no power to supervise bank-holding co...\n",
      "Pred: The Bank of Russia\n",
      "\n",
      "Q: Answer this question about Bolivia: 3.20.3 Are the following items ded...\n",
      "True: No se cuenta con normativa especÃ­fica al respecto...\n",
      "Pred: No se cuenta con normativa espec­fica al respecto\n",
      "\n",
      "Q: Answer this question about Greece: 3.20.4 Are the following items dedu...\n",
      "True: Part of the residual amount is deducted during the transitio...\n",
      "Pred: Deducted from T1\n",
      "\n",
      "Q: Answer this question about Hong Kong SAR, China: 2.2 What are the requ...\n",
      "True: A majority shareholder should provide detailed plan for the ...\n",
      "Pred: _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ _x000D_ \n",
      "\n",
      "Q: Answer this question about Honduras: 3.20.3 Are the following items de...\n",
      "True: For loans and foreclosure assets by regulation. For investme...\n",
      "Pred: Deducted from total regulatory capital\n",
      "\n",
      "Q: Answer this question about Saudi Arabia: 1.10.1 What were the primary ...\n",
      "True: Incomplete application along with not fulfilling criteria....\n",
      "Pred: Business plan not viable.\n",
      "\n",
      "Q: Answer this question about Japan: 3.20.4 Are the following items deduc...\n",
      "True: Deducted from Common Equity Tier1 capital...\n",
      "Pred: Deducted from Common Equity Tier1 capital\n",
      "\n",
      "Q: Answer this question about Curaçao: 8.23 What happens when the deposit...\n",
      "True: Do not know...\n",
      "Pred: Do not know\n",
      "\n",
      "Q: Answer this question about Benin: 3.20.4 Are the following items deduc...\n",
      "True: Interdiction de dÃ©tenir ses propres actions...\n",
      "Pred: Interdiction de dtenir ses propres actions\n",
      "\n",
      "Q: Answer this question about Senegal: 3.1 Which regulatory capital adequ...\n",
      "True: Toutes les banques et les Ã©tablissements financiers Ã  cara...\n",
      "Pred: Toutes les banques et les tablissements financiers  caract re bancaire\n",
      "\n",
      "Q: Answer this question about Germany: 3.20.1 Which of the following item...\n",
      "True: If positive, 0.6% of IRBA RWA...\n",
      "Pred: 1.25% of credit risk-weighted exposure amounts for banks using the IRB Approach for credit risk\n",
      "custom questions:\n",
      "\n",
      "Q: Answer this question about United States: What body/agency grants banking licenses?\n",
      "A: The National Banking and Securities Commission\n",
      "\n",
      "Q: Answer this question about France: What is the minimum capital requirement?\n",
      "A: The minimum capital requirement depends on the type of institution: â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“ â€“\n",
      "\n",
      "Q: Answer this question about Japan: Who regulates banks?\n",
      "A: Bank of Japan\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import random\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./flan-t5-bsae-CUSTOM-TRAINED\")\n",
    "\n",
    "test_indices = random.sample(range(len(data)), 20)\n",
    "\n",
    "for idx in test_indices:\n",
    "    sample = data[idx]\n",
    "    question = sample[\"input\"]\n",
    "    true_answer = sample[\"target\"]\n",
    "    \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    print(f\"\\nQ: {question[:70]}...\")\n",
    "    print(f\"True: {true_answer[:60]}...\")\n",
    "    print(f\"Pred: {predicted}\")\n",
    "\n",
    "# Test on custom questions\n",
    "print(\"custom questions:\")\n",
    "\n",
    "custom_questions = [\n",
    "    \"Answer this question about United States: What body/agency grants banking licenses?\",\n",
    "    \"Answer this question about France: What is the minimum capital requirement?\",\n",
    "    \"Answer this question about Japan: Who regulates banks?\"\n",
    "]\n",
    "\n",
    "for q in custom_questions:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cb337",
   "metadata": {},
   "source": [
    "Now that we have made sure the model is working, we can upload it to a server. In this project, I'm using HuggingFace as its free and allows for easy testing. For production we would use Azure/AWS/GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# you may need to run the authentication command directly in your terminal \n",
    "!pip install huggingface_hub\n",
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b285a70",
   "metadata": {},
   "source": [
    "Now that you are logged in to huggingface, you must upload the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d496a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 308M/308M [03:58<00:00, 1.29MB/s]   \n",
      "c:\\Users\\moham\\miniconda3\\envs\\wbtraining\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\moham\\.cache\\huggingface\\hub\\models--mian21--flan-t5-bsae-CUSTOM-TRAINED. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mian21/flan-t5-bsae-CUSTOM-TRAINED/commit/9269846cf7bba0c68b074278b603102fde5356e8', commit_message='Upload tokenizer', commit_description='', oid='9269846cf7bba0c68b074278b603102fde5356e8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mian21/flan-t5-bsae-CUSTOM-TRAINED', endpoint='https://huggingface.co', repo_type='model', repo_id='mian21/flan-t5-bsae-CUSTOM-TRAINED'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload model to :\n",
    "model.push_to_hub(\"mian21/flan-t5-bsae-CUSTOM-TRAINED\")\n",
    "tokenizer.push_to_hub(\"mian21/flan-t5-bsae-CUSTOM-TRAINED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844d705",
   "metadata": {},
   "source": [
    "The model can be queried directly from the huggingface server using API requests or loaded directly into your code using huggingface's autotrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d665f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Not Found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/hf-inference/models/mian21/flan-t5-bsae-CUSTOM-TRAINED\"\n",
    "headers = {\"Authorization\": \"Bearer \"}\n",
    "\n",
    "payload = {\n",
    "  \"inputs\": \"question: What body/agency grants banking licenses in the United States?\",\n",
    "  \"parameters\": {\"max_new_tokens\": 128, \"temperature\": 0.2}\n",
    "}\n",
    "\n",
    "resp = requests.post(API_URL, headers=headers, json=payload, timeout=120)\n",
    "print(resp.status_code, resp.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbtraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
